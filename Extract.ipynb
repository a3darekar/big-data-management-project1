{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e7a1c0b6-7832-4d9f-8b38-b4efcdf6f9c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fetching The 7z archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "209e2881-3ee4-42bd-a7f9-5770d551f82a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    dbutils.fs.ls(\"file:/databricks/driver/dblp.v13.7z\")\n",
    "    print(\"Archive in filesystem (file:/databricks/driver/dblp.v13.7z)\")\n",
    "except:\n",
    "    try:\n",
    "        dbutils.fs.ls(\"dbfs:/FileStore/data/dblp.v13.7z\")\n",
    "        print(\"Archive located in FileStore. Copying into local store..\")\n",
    "        dbutils.fs.cp(\"dbfs:/FileStore/data/dblp.v13.7z\", \"file:/databricks/driver/dblp.v13.7z\")\n",
    "        print(\"Completed\")\n",
    "    except:\n",
    "        print(\"7z archive not found. Fetching from URL...\")\n",
    "        !wget https://originalstatic.aminer.cn/misc/dblp.v13.7z\n",
    "        print(\"7z archive Downloaded. Moving archive to FileStore..\")\n",
    "        dbutils.fs.mkdirs(\"dbfs:/FileStore/data\")\n",
    "        dbutils.fs.cp(\"file:/databricks/driver/dblp.v13.7z\", \"dbfs:/FileStore/data/dblp.v13.7z\")\n",
    "        print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8651e6a8-f236-486a-87bb-4d97b0fdb151",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"file:/databricks/driver/dblp.v13.7z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f06f2189-223f-4267-9d22-ae64722c6fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extracting Archive into json\n",
    "\n",
    "**Skip this Section if you already have performed the extraction process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba0d13a0-0a8a-4c20-9774-c60aacedf632",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad2f9d88-78a5-40de-8ef6-6bb19c52ecd0",
     "showTitle": false,
     "title": ""
    },
    "id": "6rihFnHHC36O"
   },
   "outputs": [],
   "source": [
    "!pip install py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ac74aa7-da2a-40af-918a-f6e5e3b7a545",
     "showTitle": false,
     "title": ""
    },
    "id": "xaE3ZCOyyQd6"
   },
   "outputs": [],
   "source": [
    "import py7zr\n",
    "\n",
    "archive = py7zr.SevenZipFile('dblp.v13.7z', mode='r')\n",
    "archive.extractall()\n",
    "archive.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db87c2a2-2100-4620-b777-6c0ef4418461",
     "showTitle": false,
     "title": ""
    },
    "id": "6O-F875qxq7o"
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"file:/databricks/driver/dblpv13.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e806a652-8fcb-4de7-a022-8c05edaeab37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2. Cleaning NumberInt(#) tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e278afc1-eae9-49a1-86f0-1a791561cfa2",
     "showTitle": false,
     "title": ""
    },
    "id": "Wud9Kr2_bd4A"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Cleaning the `NumberInt` tag\n",
    "fin = open(f\"dblpv13.json\")\n",
    "fout = open(f\"dblpv13_clean.json\", \"wt\")\n",
    "for line in fin:\n",
    "    fout.write(re.sub(r\"NumberInt\\([\\d]*\\)\", lambda x: \"\".join(re.findall(r\"\\d\", x.group(0))), line))\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60c2947a-fc81-404c-be15-fe9d8cd0d002",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. Partitioning Dataset into JSON files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ec81ab9-065e-4125-8fc9-e95c12a0722c",
     "showTitle": false,
     "title": ""
    },
    "id": "K13Kt8BqTLtt"
   },
   "outputs": [],
   "source": [
    "!pip install ijson tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e5bafdf-1fa6-4e11-92b6-63fc4cfd23df",
     "showTitle": false,
     "title": ""
    },
    "id": "PqBCNju720aq"
   },
   "outputs": [],
   "source": [
    "%mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a33aec68-5fbe-4029-8d0e-defb4b409a42",
     "showTitle": false,
     "title": ""
    },
    "id": "uiAqyHl-TGfI"
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import decimal\n",
    "\n",
    "class DecimalEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, decimal.Decimal):\n",
    "            return str(o)\n",
    "        return super(DecimalEncoder, self).default(o)\n",
    "\n",
    "data_dir = 'data/'\n",
    "with open('dblpv13_clean.json', 'r') as f:\n",
    "    counter, file_id = 0, 0\n",
    "    file_buffer = []\n",
    "    for obj_data in ijson.items(f, 'item'):\n",
    "        file_buffer.append(obj_data)\n",
    "        counter += 1\n",
    "        if counter % 300000 == 0:\n",
    "            print(f\" Saving, data_PART_{file_id}.json in {data_dir}\")\n",
    "            f = open(f'{data_dir}data_PART_{file_id}.json', 'w')\n",
    "            dump = json.dumps(file_buffer, cls=DecimalEncoder)\n",
    "            f.write(dump)\n",
    "            f.close()\n",
    "            file_id += 1\n",
    "            file_buffer = []\n",
    "f = open(f'{data_dir}data_PART_{file_id}.json', 'w')\n",
    "dump = json.dumps(file_buffer, cls=DecimalEncoder)\n",
    "print(f\" Saving, data_PART_{file_id}.json in {data_dir}\")\n",
    "f.write(dump)\n",
    "f.close()\n",
    "file_id += 1\n",
    "file_buffer = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38b56dde-5178-4dd3-931f-d6955577b8ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4. Moving files to dbfs FileStore from instance storage, to make it available for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7857b3ee-940d-4f3b-ae81-35268e1a7178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# removing old json stored in filestore.\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/data/split_data/\", recurse = True)\n",
    "# Creating dir to store json in filestore..\n",
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/data/split_data\")\n",
    "# confirming dir is empty\n",
    "dbutils.fs.ls(\"dbfs:/FileStore/data/split_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6231b89d-9180-4597-888f-33f89b4469c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copying all json parts into filestore.\n",
    "dbutils.fs.cp(\"file:/databricks/driver/data/\", \"dbfs:/FileStore/data/split_data\", recurse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce0b9414-7927-49be-9f3e-aa9fcdc9fa16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading data from databricks Filestore into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d4483af4-3011-4b5b-bc35-6bea560fdba0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, ArrayType  \n",
    "\n",
    "path = \"dbfs:/FileStore/data/split_data/\"\n",
    "\n",
    "# There should be 18 files each with 300 k records. This would change if you change split value.\n",
    "file_count = len(dbutils.fs.ls(path))\n",
    "assert file_count == 18, \"Data not found. You may want to check the path or run the notebook from start again. If you updated the split value, ignore this assertion error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f774643-8974-4f92-953a-9e735689d61d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/a/42540517 (This seems more proper way to merge the frames..)\n",
    "\n",
    "\n",
    "# Currently Union fails as there seems to be difference between schemas in some instance in 9th Frame \n",
    "\n",
    "dataframes_map = map(lambda r: spark.read.option(\"inferSchema\", True).json(r), [f\"{path}data_PART_{num}.json\" for num in range(file_count)])\n",
    "union = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dataframes_map)\n",
    "union.printSchema()\n",
    "\n",
    "# dataframes = []\n",
    "# for num in range(file_count):\n",
    "#    print(f\"Reading file data_PART_{num}.json into spark\")\n",
    "#    dataframes.append(spark.read.option(\"inferSchema\", True).json(f\"{path}data_PART_{num}.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1294fe6a-de77-4455-a0b3-a8c5578c5bbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_path = \"/tmp/out\"\n",
    "\n",
    "def buildFoSTable(dataframe):\n",
    "    # pull required Fields\n",
    "    fosFrame = dataframe.select(F.explode_outer(\"fos\").alias(\"Field_of_Study\"))\n",
    "\n",
    "    # fosFrame.show()\n",
    "    # Clean (delete dups, Fill NaN values?, ...)\n",
    "    fosFrame = fosFrame.fillna(\"Unknown\")\n",
    "    fosFrame = fosFrame.distinct()\n",
    "    \n",
    "    # Append to Parquet file\n",
    "    # fos_frame.write.mode('append').parquet(\"/tmp/out/field_of_study.parquet\")\n",
    "    fosFrame.write.mode('overwrite').parquet(f\"{parquet_path}/field_of_study.parquet\")\n",
    "    \n",
    "    # pull appeneded parquet file and get distinct records\n",
    "    fosFrame = spark.read.parquet(\"/tmp/out/field_of_study.parquet\")\n",
    "    \n",
    "    # Index\n",
    "    df = fosFrame.distinct()\n",
    "    df = df.select(\"*\").withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    # return the indexed Table\n",
    "    return df.select(\"id\", \"Field_of_Study\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05fbe646-e158-4efe-9eb9-a4d836b6ccc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FoSFrame = buildFoSTable(union)\n",
    "# map the relation in Fact Table\n",
    "display(FoSFrame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a625b76-62b2-4a67-bb00-2cb17246dd2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PLAYGROUND/SANDBOX snippets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec183be6-80b5-458e-806a-8623c52af7fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "path = \"dbfs:/FileStore/data/split_data/\"\n",
    "\n",
    "first_frame = spark.read.option(\"inferSchema\", True).json(f\"{path}data_PART_0.json\")\n",
    "display(first_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49edc7f9-5c6c-42fd-9e87-de6c90c792c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be77b338-09c4-4f19-afce-e8f7f0d2038d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "authorsDF = first_frame.select(F.explode_outer(\"authors\").alias(\"authors\"))\n",
    "authorsDF = authorsDF.selectExpr(\"authors._id\", \"authors.bio\", \"authors.email\", \"authors.gid\", \"authors.name\", \"authors.name_zh\", \"authors.oid\", \"authors.oid_zh\", \"authors.orcid\", \"authors.org\", \"authors.org_zh\", \"authors.orgid\", \"authors.orgs\", \"authors.orgs_zh\", \"authors.sid\")\n",
    "\n",
    "authorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45e2b3c5-88bc-4131-a1bc-af3c88aec3f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(authorsDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a5e7c3b-cddf-43ed-9aab-af89ce9fad1b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/44831789/spark-sql-nested-withcolumn/44833112#44833112\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "first_frame = first_frame.select(F.explode_outer(\"authors\").alias(\"a\"), \"*\")\n",
    "authors_df = first_frame.selectExpr(\"a._id\", \"a.bio\", \"a.email\", \"a.gid\", \"a.name\", \"a.name_zh\", \"a.oid\", \"a.oid_zh\", \"a.orcid\", \"a.org\", \"a.org_zh\", \"a.orgid\", \"a.orgs\", \"a.orgs_zh\", \"a.sid\")\n",
    "\n",
    "display(authors_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9872fd19-0505-4213-9fc3-010cd2b2b1b2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(first_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e73892c-3067-4fdf-b83c-1293cccd3bed",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "authors_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ca1f1d47-ee93-418c-9b1d-529060e9b585",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d9e09e7-25b6-432a-8e8a-2683b1bbc12b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_frame = first_frame.select(F.explode_outer(\"fos\").alias(\"Field_of_Study\"), \"*\")\n",
    "fos_frame = first_frame.select(F.col(\"Field_of_Study\"))\n",
    "\n",
    "# fos_frame = first_frame.select(F.explode_outer(\"fos\").alias(\"Field_of_Study\"))\n",
    "# fos_frame = fos_frame.select(F.col(\"Field_of_Study\"))\n",
    "display(fos_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "515e2b9b-0f21-4ded-a797-c61928a413a2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fos_frame = fos_frame.distinct()\n",
    "fos_frame.write.parquet(\"/tmp/out/field_of_study.parquet\")\n",
    "# fos_frame.write.mode('append').parquet(\"/tmp/out/field_of_study.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c5adf7f-97dd-4c34-83b3-c292005b14a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_path = \"/tmp/out\"\n",
    "\n",
    "def buildFoSTable(dataframes):\n",
    "    for frame in dataframes:\n",
    "        # pull required Fields\n",
    "        fosFrame = frame.select(F.explode_outer(\"fos\").alias(\"Field_of_Study\"))\n",
    "\n",
    "        # fosFrame.show()\n",
    "        # Clean (delete dups, Fill NaN values?, ...)\n",
    "        fosFrame = fosFrame.fillna(\"Unknown\")\n",
    "        fosFrame = fosFrame.distinct()\n",
    "        \n",
    "        # Append to Parquet file\n",
    "        # fos_frame.write.parquet(\"/tmp/out/field_of_study.parquet\")\n",
    "        fosFrame.write.mode('append').parquet(f\"{parquet_path}/field_of_study.parquet\")\n",
    "    # pull appeneded parquet file and get distinct records\n",
    "    fosFrame = spark.read.parquet(\"/tmp/out/field_of_study.parquet\")\n",
    "    # Index\n",
    "    df = fosFrame.distinct()\n",
    "    df = df.select(\"*\").withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "    return df.select(\"id\", \"Field_of_Study\")\n",
    "    # save as table\n",
    "    # map the relation in Fact Table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33375da8-57b2-48ae-9d90-6d416a0efc28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec71bee1-cb62-479b-a930-34d9f280c5cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Merging Fact Tables..\n",
    "\n",
    "df = fos_frame.distinct()\n",
    "df = df.select(\"*\").withColumn(\"id\", F.monotonically_increasing_id())\n",
    "\n",
    "display(df.select(\"id\", \"Field_of_Study\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "89b20a71-1409-4ccc-8808-bdcbd0d78ac8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "first_frame.select(\"Field_of_Study\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "eb70cf69-49af-41d0-b67f-b0a1fa01d88c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(first_frame.filter(first_frame.Field_of_Study==df.Field_of_Study))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41d35ff7-30af-4838-a19b-28bffff0faf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/patelatharva/Data_Lake_with_Apache_Spark/blob/master/etl.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extract",
   "notebookOrigID": 2835494419911001,
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "name": "Proj 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
