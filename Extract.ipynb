{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e7a1c0b6-7832-4d9f-8b38-b4efcdf6f9c8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Fetching The 7z archive\n",
    "\n",
    "**Skip this Section if you already have performed the extraction process and jump to checkpoint for pulling data from split json files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "209e2881-3ee4-42bd-a7f9-5770d551f82a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Checking if archive is downloaded in memory.\n",
    "try:\n",
    "    dbutils.fs.ls(\"file:/databricks/driver/dblp.v13.7z\")\n",
    "    print(\"Archive in filesystem (file:/databricks/driver/dblp.v13.7z)\")\n",
    "except:\n",
    "    # If archive is not in memory, Checking databricks store for cached version and pulling into memory.\n",
    "    try:\n",
    "        dbutils.fs.ls(\"dbfs:/FileStore/data/dblp.v13.7z\")\n",
    "        print(\"Archive located in FileStore. Copying into local store..\")\n",
    "        dbutils.fs.cp(\"dbfs:/FileStore/data/dblp.v13.7z\", \"file:/databricks/driver/dblp.v13.7z\")\n",
    "        print(\"Completed\")\n",
    "    except:\n",
    "        # If archive is not cached, downloading and storing in databricks store.\n",
    "        print(\"7z archive not found. Fetching from URL...\")\n",
    "        !wget https://originalstatic.aminer.cn/misc/dblp.v13.7z\n",
    "        print(\"7z archive Downloaded. Moving archive to FileStore..\")\n",
    "        dbutils.fs.mkdirs(\"dbfs:/FileStore/data\")\n",
    "        dbutils.fs.cp(\"file:/databricks/driver/dblp.v13.7z\", \"dbfs:/FileStore/data/dblp.v13.7z\")\n",
    "        print(\"Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8651e6a8-f236-486a-87bb-4d97b0fdb151",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# The returned array should have one object of FileInfo with size =2568255035\n",
    "\n",
    "dbutils.fs.ls(\"file:/databricks/driver/dblp.v13.7z\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f06f2189-223f-4267-9d22-ae64722c6fc6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Extracting Archive into json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba0d13a0-0a8a-4c20-9774-c60aacedf632",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 1. Extracting 7zip file into 16 GB json."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad2f9d88-78a5-40de-8ef6-6bb19c52ecd0",
     "showTitle": false,
     "title": ""
    },
    "id": "6rihFnHHC36O"
   },
   "outputs": [],
   "source": [
    "!pip install py7zr -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ac74aa7-da2a-40af-918a-f6e5e3b7a545",
     "showTitle": false,
     "title": ""
    },
    "id": "xaE3ZCOyyQd6"
   },
   "outputs": [],
   "source": [
    "import py7zr\n",
    "\n",
    "archive = py7zr.SevenZipFile('dblp.v13.7z', mode='r')\n",
    "archive.extractall()\n",
    "archive.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db87c2a2-2100-4620-b777-6c0ef4418461",
     "showTitle": false,
     "title": ""
    },
    "id": "6O-F875qxq7o"
   },
   "outputs": [],
   "source": [
    "dbutils.fs.ls(\"file:/databricks/driver/dblpv13.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e806a652-8fcb-4de7-a022-8c05edaeab37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 2. Cleaning NumberInt(#) tags\n",
    "\n",
    "The json data contains non-confirming tags, and so cannot be parsed as it is. We will read each line and substitute the tag. (This should take about 25 minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e278afc1-eae9-49a1-86f0-1a791561cfa2",
     "showTitle": false,
     "title": ""
    },
    "id": "Wud9Kr2_bd4A"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Cleaning the `NumberInt` tag\n",
    "fin = open(f\"dblpv13.json\")\n",
    "fout = open(f\"dblpv13_clean.json\", \"wt\")\n",
    "for line in fin:\n",
    "    fout.write(re.sub(r\"NumberInt\\([\\d]*\\)\", lambda x: \"\".join(re.findall(r\"\\d\", x.group(0))), line))\n",
    "fin.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "60c2947a-fc81-404c-be15-fe9d8cd0d002",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 3. Partitioning Dataset into JSON files\n",
    "Since the whopping 16 GB of json data cannot be loaded into memory directly, we need to partition the data into smaller chunks (300k objects per chunk) for processing.  \n",
    "We also parse data encoded as Decimal data with DecimalEncoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2e5bafdf-1fa6-4e11-92b6-63fc4cfd23df",
     "showTitle": false,
     "title": ""
    },
    "id": "PqBCNju720aq"
   },
   "outputs": [],
   "source": [
    "%mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a33aec68-5fbe-4029-8d0e-defb4b409a42",
     "showTitle": false,
     "title": ""
    },
    "id": "uiAqyHl-TGfI"
   },
   "outputs": [],
   "source": [
    "import ijson\n",
    "import json\n",
    "import decimal\n",
    "\n",
    "class DecimalEncoder(json.JSONEncoder):\n",
    "    def default(self, o):\n",
    "        if isinstance(o, decimal.Decimal):\n",
    "            return str(o)\n",
    "        return super(DecimalEncoder, self).default(o)\n",
    "\n",
    "data_dir = 'data/'\n",
    "with open('dblpv13_clean.json', 'r') as f:\n",
    "    counter, file_id = 0, 0\n",
    "    file_buffer = []\n",
    "    for obj_data in ijson.items(f, 'item'):\n",
    "        file_buffer.append(obj_data)\n",
    "        counter += 1\n",
    "        if counter % 300000 == 0:\n",
    "            print(f\" Saving, data_PART_{file_id}.json in {data_dir}\")\n",
    "            f = open(f'{data_dir}data_PART_{file_id}.json', 'w')\n",
    "            dump = json.dumps(file_buffer, cls=DecimalEncoder)\n",
    "            f.write(dump)\n",
    "            f.close()\n",
    "            file_id += 1\n",
    "            file_buffer = []\n",
    "f = open(f'{data_dir}data_PART_{file_id}.json', 'w')\n",
    "dump = json.dumps(file_buffer, cls=DecimalEncoder)\n",
    "print(f\" Saving, data_PART_{file_id}.json in {data_dir}\")\n",
    "f.write(dump)\n",
    "f.close()\n",
    "file_id += 1\n",
    "file_buffer = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "38b56dde-5178-4dd3-931f-d6955577b8ea",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "#### 4. Moving files to dbfs FileStore from instance storage, to make it available for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7857b3ee-940d-4f3b-ae81-35268e1a7178",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# removing old json stored in filestore.\n",
    "dbutils.fs.rm(\"dbfs:/FileStore/data/split_data/\", recurse = True)\n",
    "# Creating dir to store json in filestore..\n",
    "dbutils.fs.mkdirs(\"dbfs:/FileStore/data/split_data\")\n",
    "# confirming dir is empty\n",
    "dbutils.fs.ls(\"dbfs:/FileStore/data/split_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6231b89d-9180-4597-888f-33f89b4469c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Copying all json parts into filestore.\n",
    "dbutils.fs.cp(\"file:/databricks/driver/data/\", \"dbfs:/FileStore/data/split_data\", recurse = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4a08e500-0a30-4c03-a776-58f12706a511",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Checkpoint after data load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ce0b9414-7927-49be-9f3e-aa9fcdc9fa16",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Reading data from databricks Filestore into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d4483af4-3011-4b5b-bc35-6bea560fdba0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructType, ArrayType  \n",
    "\n",
    "path = \"dbfs:/FileStore/data/split_data/\"\n",
    "\n",
    "# There should be 18 files each with 300 k records. This would change if you change split value.\n",
    "file_count = len(dbutils.fs.ls(path))\n",
    "assert file_count == 18, \"Data not found. You may want to check the path or run the notebook from start again. If you updated the split value, ignore this assertion error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f774643-8974-4f92-953a-9e735689d61d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Build map of spark dataframes by reading json partition chunk files\n",
    "dataframes_map = map(lambda r: spark.read.option(\"inferSchema\", True).json(r), [f\"{path}data_PART_{num}.json\" for num in range(file_count)])\n",
    "# reduce the dataframes into single dataframe by performing union over the mapped frames.\n",
    "union = reduce(lambda df1, df2: df1.unionByName(df2, allowMissingColumns=True), dataframes_map)\n",
    "union.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1294fe6a-de77-4455-a0b3-a8c5578c5bbd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parquet_path = \"/tmp/out\"\n",
    "# dbutils.fs.rm(f\"{parquet_path}\", recurse = True)\n",
    "\n",
    "def buildFoSTable(dataframe):\n",
    "    # pull required Fields\n",
    "    fosFrame = dataframe.select(F.explode_outer(\"fos\").alias(\"Field_of_Study\"))\n",
    "\n",
    "    # fosFrame.show()\n",
    "    # Clean (delete dups, Fill NaN values?, ...)\n",
    "    fosFrame = fosFrame.fillna(\"Unknown\")\n",
    "    fosFrame = fosFrame.distinct()\n",
    "    \n",
    "    # Append to Parquet file\n",
    "    # fos_frame.write.mode('append').parquet(\"/tmp/out/field_of_study.parquet\")\n",
    "    fosFrame.write.mode('overwrite').parquet(f\"{parquet_path}/field_of_study.parquet\")\n",
    "    \n",
    "    # pull appeneded parquet file and get distinct records\n",
    "    fosFrame = spark.read.parquet(\"/tmp/out/field_of_study.parquet\")\n",
    "    \n",
    "    # Index\n",
    "    df = fosFrame.distinct()\n",
    "    df = df.select(\"*\").withColumn(\"id\", F.monotonically_increasing_id())\n",
    "    # return the indexed Table\n",
    "    return df.select(\"id\", \"Field_of_Study\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "05fbe646-e158-4efe-9eb9-a4d836b6ccc2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FoSFrame = buildFoSTable(union)\n",
    "# map the relation in Fact Table\n",
    "display(FoSFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a26c5c64-0762-436d-8b03-96787cf96b08",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "FoSFrame = spark.read.parquet(\"/tmp/out/field_of_study.parquet\")\n",
    "FoSFrame = FoSFrame.select(\"*\").withColumn(\"id\", F.monotonically_increasing_id())\n",
    "FoSrdd = FoSFrame.rdd.collectAsMap()\n",
    "FoSrdd_map = F.map_from_arrays(\n",
    "    F.array(*map(F.lit, FoSrdd.keys())),\n",
    "    F.array(*map(F.lit, FoSrdd.values()))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cda75f1e-046f-4699-b9a6-dba03e6713f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fosUnion = union.select(F.explode_outer(\"fos\").alias(\"Field_of_Study\"), '*')\n",
    "fosUnion.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "78572ef8-5eef-4aba-bd2c-0986e60d5266",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fosUnion = fosUnion.withColumn('FoS_fk', FoSrdd_map.getItem(F.col('Field_of_Study')))\n",
    "fosUnion = fosUnion.withColumn('FoS_fk', F.when(F.col('FoS_fk').isNull(), 0).otherwise(F.col('FoS_fk')))\n",
    "\n",
    "display(fosUnion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2190b5a1-8cb7-4dbe-a852-ef19f9305934",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fosUnion = fosUnion.join(\n",
    "    F.broadcast(FoSFrame.select(\"*\")), \n",
    "    \"FoS_fk\" == FoSFrame.id,\n",
    "    how=\"left\"\n",
    ").select(\"FoS_fk\", \"_id\", \"id\", \"Field_of_Study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5ba81066-cc66-488f-ad5c-54d500fc82b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(fosUnion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3a625b76-62b2-4a67-bb00-2cb17246dd2f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## PLAYGROUND/SANDBOX snippets\n",
    "\n",
    "Testing scripts while Working with single chunk to reduce processing time.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec183be6-80b5-458e-806a-8623c52af7fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Data path:\n",
    "path = \"dbfs:/FileStore/data/split_data/\"\n",
    "\n",
    "# Reading first chunk\n",
    "first_frame = spark.read.option(\"inferSchema\", True).json(f\"{path}data_PART_0.json\")\n",
    "first_frame.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49edc7f9-5c6c-42fd-9e87-de6c90c792c4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Looking at schema\n",
    "first_frame.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be77b338-09c4-4f19-afce-e8f7f0d2038d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Extracting Authors from the dataset\n",
    "\n",
    "# Exploding a column returns a new row for each element in the given array or map type. \n",
    "# For each item in the map/array of data it creates a copy of the row and with that element in new column.\n",
    "# Here, We only select the exploded column, and so we only get row with author object in the generated frame.\n",
    "authorsDF = first_frame.select(F.explode_outer(\"authors\").alias(\"authors\"))\n",
    "\n",
    "\n",
    "# selectExpr Projects a set of SQL expressions and returns a new DataFrame. e.g. (authors['name', 'email'] => [authors.name, authors.email])\n",
    "\n",
    "authorsDF = authorsDF.selectExpr(\"authors._id\", \"authors.bio\", \"authors.email\", \"authors.gid\", \"authors.name\", \"authors.name_zh\", \"authors.oid\", \"authors.oid_zh\", \"authors.orcid\", \"authors.org\", \"authors.org_zh\", \"authors.orgid\", \"authors.orgs\", \"authors.orgs_zh\", \"authors.sid\")\n",
    "\n",
    "authorsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "45e2b3c5-88bc-4131-a1bc-af3c88aec3f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "authorsDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "41d35ff7-30af-4838-a19b-28bffff0faf8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# https://github.com/patelatharva/Data_Lake_with_Apache_Spark/blob/master/etl.py"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Extract",
   "notebookOrigID": 2835494419911001,
   "widgets": {}
  },
  "colab": {
   "collapsed_sections": [],
   "name": "Proj 1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
